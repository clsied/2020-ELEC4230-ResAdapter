{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import BertTokenizer,squad_convert_examples_to_features, AutoConfig, AutoModelForQuestionAnswering\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "#from ProbeAttention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'bert-base-uncased'\n",
    "model_prefix = 'bert-base-uncased'\n",
    "data_dir = ''\n",
    "data_file = 'train-v2.0.json'\n",
    "max_seq_length = 384\n",
    "res_size = 3\n",
    "non_linear = \"gelu\"\n",
    "project_dim = 200\n",
    "layers = 12\n",
    "hidden_dim = 768\n",
    "epochs = 5\n",
    "batch_size = 8\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 0.1\n",
    "dropout_r = 0.3\n",
    "lr = 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "if torch.cuda.is_available():       \n",
    "    device = 'cuda'\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = 'cpu'\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_prefix)\n",
    "\n",
    "# Extract examples\n",
    "processor = SquadV2Processor()\n",
    "train_examples = processor.get_train_examples(data_dir=data_dir, filename=data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train features\n",
    "print(\"Loading train features\")\n",
    "train_features, train_dataset = squad_convert_examples_to_features(\n",
    "    examples=train_examples[0:50000],\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=128,\n",
    "    max_query_length=64,\n",
    "    is_training=True,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "#from ProbeAttention import *\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_prefix, output_hidden_states = True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_prefix, config = config)\n",
    "\n",
    "# multi-gpu evaluate one at here\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_dim, n_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()   \n",
    "        assert in_dim % n_heads == 0\n",
    "        self.d = in_dim//n_heads\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.WQ = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        self.WK = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        self.WV = nn.Linear(in_dim, self.d * self.n_heads)\n",
    "        \n",
    "        self.linear = nn.Linear(self.n_heads * self.d, in_dim)\n",
    "        self.layer_norm = nn.LayerNorm(in_dim)\n",
    "        \n",
    "    def forward(self, h): # (8, 384, 200)\n",
    "        batch_size = h.shape[0]\n",
    "        q_s = self.WQ(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2) # (8, 4, 384, 50)\n",
    "        k_s = self.WK(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2)\n",
    "        v_s = self.WV(h).view(batch_size, -1, self.n_heads, self.d).transpose(1, 2) \n",
    "\n",
    "        scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / np.sqrt(self.d) #(8, 4, 384, 384)\n",
    "        attn = F.softmax(scores, dim=-1) \n",
    "        context = torch.matmul(attn, v_s) #(8, 4, 384, 50)\n",
    "\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d)\n",
    "        output = self.linear(context)\n",
    "        return self.layer_norm(output + h)\n",
    "        \n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, in_dim, project_dim, non_linear, p = dropout_r, max_seq = max_seq_length):\n",
    "        super(Adapter, self).__init__()        \n",
    "        assert non_linear in [\"relu\",\"gelu\",\"tanh\"]\n",
    "        \n",
    "        if (non_linear == \"relu\"):\n",
    "            self.non_linear = nn.ReLU()\n",
    "        elif (non_linear == \"gelu\"):\n",
    "            self.non_linear = nn.GELU()\n",
    "        elif (non_linear == \"tanh\"):\n",
    "            self.non_linear = nn.Tanh()\n",
    "                \n",
    "        self.project_down = nn.Linear(in_dim, project_dim)\n",
    "        self.project_up = nn.Linear(project_dim, in_dim)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.batchnorm = nn.BatchNorm1d(max_seq)\n",
    "        self.layernorm = nn.LayerNorm(in_dim, max_seq)\n",
    "        self.attention = MultiHeadAttention(project_dim)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h = self.project_down(h)\n",
    "        h = self.batchnorm(h)\n",
    "        h = self.attention(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.project_up(h)\n",
    "        h = self.layernorm(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class ResAdapter(nn.Module):\n",
    "    def __init__(self, in_dim, project_dim, non_linear, res_size, max_seq=max_seq_length):\n",
    "        super(ResAdapter, self).__init__()\n",
    "        \n",
    "        self.res_size = res_size\n",
    "        self.adapter_list = nn.ModuleList([Adapter(in_dim, project_dim, non_linear) for i in range(res_size)])\n",
    "        \n",
    "    def forward(self, h_list, h_last=None):\n",
    "        h = torch.zeros(h_list[0].size()).to(device)\n",
    "        \n",
    "        if (h_last != None):\n",
    "            h = h_last\n",
    "            \n",
    "        for i in range(res_size):\n",
    "            if (i == 0):\n",
    "                h = self.adapter_list[i](h_list[i]+h) + h_list[i+1]\n",
    "            elif (i==res_size-1):\n",
    "                h = self.adapter_list[i](h) + h_list[0]\n",
    "            else:\n",
    "                h = self.adapter_list[i](h) + h_list[i+1]\n",
    "        \n",
    "        return h\n",
    "\n",
    "class ResAdapterModel(nn.Module):\n",
    "    def __init__(self, in_dim, project_dim, non_linear, res_size, max_seq):\n",
    "        super(ResAdapterModel, self).__init__()\n",
    "        assert (12 % res_size == 0)\n",
    "        \n",
    "        self.res_size = res_size\n",
    "        self.res_list = nn.ModuleList([ResAdapter(in_dim, project_dim, non_linear, res_size) for i in range(12//res_size)])\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "    \n",
    "    def forward(self, all_h):\n",
    "        h = torch.zeros(all_h[0].size()).to(device)\n",
    "        \n",
    "        h_list = []\n",
    "        for i in range(12):\n",
    "            h_list.append(all_h[i])\n",
    "            if (i%res_size==res_size-1):\n",
    "                h = self.res_list[i//res_size](h_list, h)\n",
    "                h_list = []\n",
    "        return self.linear(h).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize adaptors\n",
    "print(\"Initializing adaptors\")\n",
    "adaptor_s = ResAdapterModel(hidden_dim, project_dim, non_linear, res_size, max_seq_length)\n",
    "adaptor_e = ResAdapterModel(hidden_dim, project_dim, non_linear, res_size, max_seq_length)\n",
    "\n",
    "adaptor_s.to(device)\n",
    "adaptor_e.to(device)\n",
    "\n",
    "start_optimizer = AdamW(adaptor_s.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)\n",
    "end_optimizer = AdamW(adaptor_e.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum loss\n",
    "min_loss = 1000000000\n",
    "    \n",
    "# start & end hidden state\n",
    "start_hidden={}\n",
    "end_hidden={}\n",
    "\n",
    "# create results folder\n",
    "if not os.path.exists('results'):\n",
    "        os.mkdir('results')\n",
    "        \n",
    "if not os.path.exists('results'+'/'+ model_prefix):\n",
    "        os.mkdir('results'+'/'+ model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training epochs\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # create each epochs folder    \n",
    "    epoch_dir = \"results\"+\"/\"+ model_prefix + \"/\"+\"epoch_\" + str(epoch + 1)\n",
    "    if not os.path.exists(epoch_dir):\n",
    "        os.mkdir(epoch_dir)\n",
    "    \n",
    "    print(\"Training epoch: {}\".format(epoch+1))\n",
    "    adaptor_s.train()\n",
    "    adaptor_e.train()\n",
    "\n",
    "    # Track epoch loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Initialize train data loader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = batch_size)\n",
    "    loss_list = []\n",
    "    \n",
    "    iter=0\n",
    "    for batch in tqdm(train_dataloader, desc = \"Iteration\"):\n",
    "        \n",
    "        # Get batch on the right device and prepare input dict\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"token_type_ids\": batch[2],\n",
    "            \"start_positions\": batch[3],\n",
    "            \"end_positions\": batch[4],\n",
    "        }\n",
    "\n",
    "        # BERT forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract hiddent states\n",
    "        all_layer_hidden_states = outputs[3][1:] # (layers, batch_size, max_seq_len, hidden_size)\n",
    "        # Get labels, and update probes for batch\n",
    "        start_targets = batch[3] # (batch_size)\n",
    "        end_targets  = batch[4] # (batch_size)\n",
    "        \n",
    "        # in: (layers, batch_size, max_seq_len, hidden_size)\n",
    "        # out: (layers, batch_size, max_seq_len, 1)\n",
    "#         print(torch.stack(list(all_layer_hidden_states)).size())# ->ã€€torch.Size([12, 8, 384, 768])\n",
    "        s_scores = adaptor_s(list(all_layer_hidden_states)).squeeze() \n",
    "#         print(s_scores.size())\n",
    "        e_scores = adaptor_e(list(all_layer_hidden_states)).squeeze()      \n",
    "        \n",
    "        ignored_index = s_scores.size(1)\n",
    "        size_of_batch = s_scores.size(0)\n",
    "        # print(ignored_index) 384\n",
    "        s_scores.clamp_(0, ignored_index) # (8, 384)\n",
    "        e_scores.clamp_(0, ignored_index) # (8, 384)\n",
    "        \n",
    "        start_loss = nn.CrossEntropyLoss(\n",
    "            weight = None, ignore_index=ignored_index)(s_scores, start_targets)\n",
    "        end_loss = nn.CrossEntropyLoss(\n",
    "            weight = None, ignore_index=ignored_index)(e_scores, end_targets)\n",
    "\n",
    "        loss = (1.5*start_loss+end_loss)/2.5\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            adaptor_s.parameters(), max_grad_norm)\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            adaptor_e.parameters(), max_grad_norm)\n",
    "\n",
    "        start_optimizer.step()\n",
    "        end_optimizer.step()\n",
    "\n",
    "        adaptor_s.zero_grad()\n",
    "        adaptor_e.zero_grad()\n",
    "        \n",
    "        batch_loss = float(loss)\n",
    "        loss_list.append(batch_loss)\n",
    "        \n",
    "        if (iter != 0 and iter%100==0):\n",
    "            plt.plot(loss_list)\n",
    "            plt.show()\n",
    "            plt.plot(loss_list,'o')\n",
    "            plt.show()\n",
    "        \n",
    "        epoch_loss+=batch_loss\n",
    "        iter+=1\n",
    "        \n",
    "        # store min loss's hidden state\n",
    "        if iter % 100 == 0:\n",
    "            torch.save(adaptor_s.state_dict(), epoch_dir + \"/\" + \"_start_idx_per100\")\n",
    "            torch.save(adaptor_e.state_dict(), epoch_dir + \"/\" + \"_end_idx_per100\")\n",
    "        if batch_loss < min_loss:\n",
    "            torch.save(adaptor_s.state_dict(), epoch_dir + \"/\" + \"_start_idx\")\n",
    "            torch.save(adaptor_e.state_dict(), epoch_dir + \"/\" + \"_end_idx\")\n",
    "            min_loss = batch_loss        \n",
    "\n",
    "    print(\"Epoch loss {}\".format(epoch_loss))\n",
    "    print(\"Epoch {} complete, saving probes\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
